{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyMSxEcsf2B6LsvV7JpSRfRY",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Subhanshu-2411/Neural-Machine-Translation/blob/main/Neural_Machine_Translation_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# IMPORTING REQUIRED LIBRARIES"
   ],
   "metadata": {
    "id": "xic9I95g5AuL"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "W8NCunGZ3SPy",
    "ExecuteTime": {
     "end_time": "2023-05-06T07:40:44.468629500Z",
     "start_time": "2023-05-06T07:40:44.409751100Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from pickle import dump, load\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "from numpy.random import rand, shuffle\n",
    "# from google.colab import drive\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# drive.mount('/gdrive')\n",
    "# %cd /gdrive"
   ],
   "metadata": {
    "id": "CGdgQSL94y3S",
    "ExecuteTime": {
     "end_time": "2023-05-06T07:29:34.234768100Z",
     "start_time": "2023-05-06T07:29:34.175506100Z"
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text"
   ],
   "metadata": {
    "id": "gkI4fPh44y0a",
    "ExecuteTime": {
     "end_time": "2023-05-06T07:29:34.280409Z",
     "start_time": "2023-05-06T07:29:34.187317500Z"
    }
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# split a loaded document into sentences\n",
    "def to_pairs(doc):\n",
    "\tlines = doc.strip().split('\\n')\n",
    "\tpairs = [line.split('\\t') for line in  lines]\n",
    "\treturn pairs"
   ],
   "metadata": {
    "id": "1vsTJt76410q",
    "ExecuteTime": {
     "end_time": "2023-05-06T07:29:34.281409900Z",
     "start_time": "2023-05-06T07:29:34.195532400Z"
    }
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# clean a list of lines\n",
    "def clean_pairs(lines):\n",
    "\tcleaned = list()\n",
    "\t# prepare regex for char filtering\n",
    "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "\t# prepare translation table for removing punctuation\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\tfor pair in lines:\n",
    "\t\tclean_pair = list()\n",
    "\t\tfor line in pair:\n",
    "\t\t\t# normalize unicode characters\n",
    "\t\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "\t\t\tline = line.decode('UTF-8')\n",
    "\t\t\t# tokenize on white space\n",
    "\t\t\tline = line.split()\n",
    "\t\t\t# convert to lowercase\n",
    "\t\t\tline = [word.lower() for word in line]\n",
    "\t\t\t# remove punctuation from each token\n",
    "\t\t\tline = [word.translate(table) for word in line]\n",
    "\t\t\t# remove non-printable chars form each token\n",
    "\t\t\tline = [re_print.sub('', w) for w in line]\n",
    "\t\t\t# remove tokens with numbers in them\n",
    "\t\t\tline = [word for word in line if word.isalpha()]\n",
    "\t\t\t# store as string\n",
    "\t\t\tclean_pair.append(' '.join(line))\n",
    "\t\tcleaned.append(clean_pair)\n",
    "\treturn array(cleaned)"
   ],
   "metadata": {
    "id": "vZtpn3GA44sK",
    "ExecuteTime": {
     "end_time": "2023-05-06T07:29:34.284412500Z",
     "start_time": "2023-05-06T07:29:34.205643700Z"
    }
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    " dump(sentences, open(filename, 'wb'))\n",
    " print('Saved: %s' % filename)"
   ],
   "metadata": {
    "id": "2Xd0_g9Q7tBp",
    "ExecuteTime": {
     "end_time": "2023-05-06T07:29:34.300528400Z",
     "start_time": "2023-05-06T07:29:34.276865500Z"
    }
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# load dataset\n",
    "# folder_path = \"/gdrive/My Drive/Project Data/Blog_Projects/Machine Learning Mastery/\"\n",
    "folder_path = \"./data/\"\n",
    "filename = os.path.join(folder_path, 'deu-eng.txt')\n",
    "doc = load_doc(filename)\n",
    "# split into english-german pairs\n",
    "pairs = to_pairs(doc)\n",
    "# clean sentences\n",
    "clean_pairs = clean_pairs(pairs)\n",
    "# save clean pairs to file\n",
    "save_clean_data(clean_pairs, 'data/english-german.pkl')\n",
    "# spot check\n",
    "for i in range(100):\n",
    " print('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
   ],
   "metadata": {
    "id": "LfwCI4tV5QTQ",
    "ExecuteTime": {
     "end_time": "2023-05-06T07:29:47.324531500Z",
     "start_time": "2023-05-06T07:29:34.281409900Z"
    }
   },
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/ende.pkl\n",
      "[hi] => [hallo]\n",
      "[hi] => [gru gott]\n",
      "[run] => [lauf]\n",
      "[wow] => [potzdonner]\n",
      "[wow] => [donnerwetter]\n",
      "[fire] => [feuer]\n",
      "[help] => [hilfe]\n",
      "[help] => [zu hulf]\n",
      "[stop] => [stopp]\n",
      "[wait] => [warte]\n",
      "[hello] => [hallo]\n",
      "[i try] => [ich probiere es]\n",
      "[i won] => [ich hab gewonnen]\n",
      "[i won] => [ich habe gewonnen]\n",
      "[smile] => [lacheln]\n",
      "[cheers] => [zum wohl]\n",
      "[freeze] => [keine bewegung]\n",
      "[freeze] => [stehenbleiben]\n",
      "[got it] => [verstanden]\n",
      "[got it] => [einverstanden]\n",
      "[he ran] => [er rannte]\n",
      "[he ran] => [er lief]\n",
      "[hop in] => [mach mit]\n",
      "[hug me] => [druck mich]\n",
      "[hug me] => [nimm mich in den arm]\n",
      "[hug me] => [umarme mich]\n",
      "[i fell] => [ich fiel]\n",
      "[i fell] => [ich fiel hin]\n",
      "[i fell] => [ich sturzte]\n",
      "[i fell] => [ich bin hingefallen]\n",
      "[i fell] => [ich bin gesturzt]\n",
      "[i know] => [ich wei]\n",
      "[i lied] => [ich habe gelogen]\n",
      "[i lost] => [ich habe verloren]\n",
      "[im] => [ich bin jahre alt]\n",
      "[im] => [ich bin]\n",
      "[im ok] => [mir gehts gut]\n",
      "[im ok] => [es geht mir gut]\n",
      "[no way] => [unmoglich]\n",
      "[no way] => [das gibts doch nicht]\n",
      "[no way] => [ausgeschlossen]\n",
      "[no way] => [in keinster weise]\n",
      "[really] => [wirklich]\n",
      "[really] => [echt]\n",
      "[really] => [im ernst]\n",
      "[thanks] => [danke]\n",
      "[try it] => [versuchs]\n",
      "[why me] => [warum ich]\n",
      "[ask tom] => [frag tom]\n",
      "[ask tom] => [fragen sie tom]\n",
      "[ask tom] => [fragt tom]\n",
      "[be cool] => [entspann dich]\n",
      "[be fair] => [sei nicht ungerecht]\n",
      "[be fair] => [sei fair]\n",
      "[be nice] => [sei nett]\n",
      "[be nice] => [seien sie nett]\n",
      "[beat it] => [geh weg]\n",
      "[beat it] => [hau ab]\n",
      "[beat it] => [verschwinde]\n",
      "[beat it] => [verdufte]\n",
      "[beat it] => [mach dich fort]\n",
      "[beat it] => [zieh leine]\n",
      "[beat it] => [mach dich vom acker]\n",
      "[beat it] => [verzieh dich]\n",
      "[beat it] => [verkrumele dich]\n",
      "[beat it] => [troll dich]\n",
      "[beat it] => [zisch ab]\n",
      "[beat it] => [pack dich]\n",
      "[beat it] => [mach ne fliege]\n",
      "[beat it] => [schwirr ab]\n",
      "[beat it] => [mach die sause]\n",
      "[beat it] => [scher dich weg]\n",
      "[beat it] => [scher dich fort]\n",
      "[call me] => [ruf mich an]\n",
      "[come in] => [komm herein]\n",
      "[come in] => [herein]\n",
      "[come on] => [komm]\n",
      "[come on] => [kommt]\n",
      "[come on] => [mach schon]\n",
      "[come on] => [macht schon]\n",
      "[get out] => [raus]\n",
      "[go away] => [geh weg]\n",
      "[go away] => [hau ab]\n",
      "[go away] => [verschwinde]\n",
      "[go away] => [verdufte]\n",
      "[go away] => [mach dich fort]\n",
      "[go away] => [zieh leine]\n",
      "[go away] => [mach dich vom acker]\n",
      "[go away] => [verzieh dich]\n",
      "[go away] => [verkrumele dich]\n",
      "[go away] => [troll dich]\n",
      "[go away] => [zisch ab]\n",
      "[go away] => [pack dich]\n",
      "[go away] => [mach ne fliege]\n",
      "[go away] => [schwirr ab]\n",
      "[go away] => [mach die sause]\n",
      "[go away] => [scher dich weg]\n",
      "[go away] => [scher dich fort]\n",
      "[go away] => [geh weg]\n",
      "[go away] => [verpiss dich]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "\treturn load(open(filename,\"rb\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T07:29:47.330667200Z",
     "start_time": "2023-05-06T07:29:47.327066200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "\tdump(sentences, open(filename, \"wb\"))\n",
    "\tprint(\"Saved: %s\" % filename)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T08:09:18.036642200Z",
     "start_time": "2023-05-06T08:09:18.028068400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_clean_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# load dataset\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mload_clean_sentences\u001B[49m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata/english-german.pkl\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'load_clean_sentences' is not defined"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = load_clean_sentences('data/english-german.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T08:09:18.738275700Z",
     "start_time": "2023-05-06T08:09:18.719992100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# random shuffle\n",
    "shuffle(dataset)\n",
    "# index = [0.6, 0.7, 0.8, 0.9]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_clean_data(dataset, \"data/english-german-both.plk\")\n",
    "# for i in index:\n",
    "# \ttrain_index = len(dataset) * i\n",
    "# \t# split into train / test\n",
    "# \ttrain, test = dataset[:train_index], dataset[train_index:]\n",
    "# \t# save data\n",
    "# \tsave_clean_data(train, f\"data/english-german-train-{i * 100}-{(1 - i) * 100}\")\n",
    "# \tsave_clean_data(test, f\"data/english-german-test-{i * 100}-{(1 - i) * 100}\")\n",
    "index = len(dataset) * 0.7\n",
    "\n",
    "train, test = dataset[:index], dataset[index:]\n",
    "\n",
    "save_clean_data(train, \"data/english-german-train.pkl\")\n",
    "save_clean_data(test, \"data/english-german-test.pkl\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = load_clean_sentences(\"english-german-both.pkl\")\n",
    "# train_60_40 = load_clean_sentences(\"data/english-german-train-60-40.pkl\")\n",
    "# test_60_40 = load_clean_sentences(\"data/english-german-test-60-40.pkl\")\n",
    "# train_70_30 = load_clean_sentences(\"data/english-german-train-70-30.pkl\")\n",
    "# test_70_30 = load_clean_sentences(\"data/english-german-test-70-30.pkl\")\n",
    "# train_80_20 = load_clean_sentences(\"data/english-german-train-80-20.pkl\")\n",
    "# test_80_20 = load_clean_sentences(\"data/english-german-test-80-20.pkl\")\n",
    "# train_90_10 = load_clean_sentences(\"data/english-german-train-90-10.pkl\")\n",
    "# test_90_10 = load_clean_sentences(\"data/english-german-test-90-10.pkl\")\n",
    "train = load_clean_sentences(\"data/english-german-train.pkl\")\n",
    "test = load_clean_sentences(\"data/english-german-test.pkl\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# max sentence length\n",
    "def max_length(lines):\n",
    "\treturn max(len(line.split()) for line in lines)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# prepare english tokenizer\n",
    "english_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "english_vocabulary_size = len(english_tokenizer.word_index) + 1\n",
    "english_length = max_length(dataset[:, 0])\n",
    "print(\"English Vocabulary Size: %d\" % english_vocabulary_size)\n",
    "print(\"English Max Length: %d\" %(english_length))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# prepare german tokenizer\n",
    "german_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "german_vocabulary_size = len(german_tokenizer.word_index) + 1\n",
    "german_length = max_length(dataset[:, 1])\n",
    "print(\"German Vocabulary Size: %d\" % german_vocabulary_size)\n",
    "print(\"German Max Length: %d\" %(german_length))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# encode and padding sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "\t# integer encode sequences\n",
    "\tX = tokenizer.texts_to_sequences(lines)\n",
    "\t# padding sequences with 0 values\n",
    "\tX = pad_sequences(X, max_length=length, padding='post')\n",
    "\treturn X"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# one hot encode target sequence\n",
    "def encode_output(sequences, vocab_size):\n",
    "\tylist = list()\n",
    "\tfor sequence in sequences:\n",
    "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "\t\tylist.append(encoded)\n",
    "\ty = array(ylist)\n",
    "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "\treturn y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
