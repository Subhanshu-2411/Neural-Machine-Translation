{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyMSxEcsf2B6LsvV7JpSRfRY",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Subhanshu-2411/Neural-Machine-Translation/blob/main/Neural_Machine_Translation_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# IMPORTING REQUIRED LIBRARIES"
   ],
   "metadata": {
    "id": "xic9I95g5AuL"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W8NCunGZ3SPy"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "from google.colab import drive\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "drive.mount('/gdrive')\n",
    "%cd /gdrive"
   ],
   "metadata": {
    "id": "CGdgQSL94y3S"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text"
   ],
   "metadata": {
    "id": "gkI4fPh44y0a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# split a loaded document into sentences\n",
    "def to_pairs(doc):\n",
    "\tlines = doc.strip().split('\\n')\n",
    "\tpairs = [line.split('\\t') for line in  lines]\n",
    "\treturn pairs"
   ],
   "metadata": {
    "id": "1vsTJt76410q"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# clean a list of lines\n",
    "def clean_pairs(lines):\n",
    "\tcleaned = list()\n",
    "\t# prepare regex for char filtering\n",
    "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "\t# prepare translation table for removing punctuation\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\tfor pair in lines:\n",
    "\t\tclean_pair = list()\n",
    "\t\tfor line in pair:\n",
    "\t\t\t# normalize unicode characters\n",
    "\t\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "\t\t\tline = line.decode('UTF-8')\n",
    "\t\t\t# tokenize on white space\n",
    "\t\t\tline = line.split()\n",
    "\t\t\t# convert to lowercase\n",
    "\t\t\tline = [word.lower() for word in line]\n",
    "\t\t\t# remove punctuation from each token\n",
    "\t\t\tline = [word.translate(table) for word in line]\n",
    "\t\t\t# remove non-printable chars form each token\n",
    "\t\t\tline = [re_print.sub('', w) for w in line]\n",
    "\t\t\t# remove tokens with numbers in them\n",
    "\t\t\tline = [word for word in line if word.isalpha()]\n",
    "\t\t\t# store as string\n",
    "\t\t\tclean_pair.append(' '.join(line))\n",
    "\t\tcleaned.append(clean_pair)\n",
    "\treturn array(cleaned)"
   ],
   "metadata": {
    "id": "vZtpn3GA44sK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    " dump(sentences, open(filename, 'wb'))\n",
    " print('Saved: %s' % filename)"
   ],
   "metadata": {
    "id": "2Xd0_g9Q7tBp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# load dataset\n",
    "folder_path = \"/gdrive/My Drive/Project Data/Blog_Projects/Machine Learning Mastery/\"\n",
    "filename = os.path.join(folder_path, 'deu.txt')\n",
    "doc = load_doc(filename)\n",
    "# split into english-german pairs\n",
    "pairs = to_pairs(doc)\n",
    "# clean sentences\n",
    "clean_pairs = clean_pairs(pairs)\n",
    "# save clean pairs to file\n",
    "save_clean_data(clean_pairs, 'ende.pkl')\n",
    "# spot check\n",
    "for i in range(100):\n",
    " print('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
   ],
   "metadata": {
    "id": "LfwCI4tV5QTQ"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
